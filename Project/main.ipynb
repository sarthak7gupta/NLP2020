{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import contractions\n",
    "import emoji\n",
    "import en_core_web_sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn import svm, tree\n",
    "from sklearn.ensemble import AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "nlp = en_core_web_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "deselect_stop_words = ['no', 'not']\n",
    "\n",
    "for w in deselect_stop_words: nlp.vocab[w].is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_REGEX = re.compile(r\"(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?\", re.UNICODE)\n",
    "TAGS_REGEX = re.compile(r\"(@[A-Za-z0-9]+)\", re.UNICODE)\n",
    "\n",
    "def cleanse_tags_mentions(text):\n",
    "    return TAGS_REGEX.sub(r\" \", text)\n",
    "\n",
    "def cleanse_links(text):\n",
    "    return URL_REGEX.sub(r' ',text)\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    # cafÃ© -> cafe\n",
    "    return unidecode.unidecode(text)\n",
    "\n",
    "def expand_contractions(text):\n",
    "    # don't -> do not\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_excess_whitespace(text):\n",
    "    # '  ' -> ' '\n",
    "    return \" \".join(text.strip().split())\n",
    "\n",
    "def extract_emojis(sentence):\n",
    "    return [char for char in sentence if str(char.encode('unicode-escape'))[2] == '\\\\' ]\n",
    "\n",
    "def char_is_emoji(character):\n",
    "    return character in emoji.UNICODE_EMOJI\n",
    "\n",
    "def spell_correct(text):\n",
    "    text = text.split()\n",
    "    wrong = spell.unknown(text)\n",
    "    return ' '.join(spell.correction(word) if word in wrong and word.isalpha() and not char_is_emoji(word) else word for word in text)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return ' '.join(word for word in text.split() if not nlp.vocab[word].is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = ['Final_Dataset_Facebook.csv', 'Final_Dataset_Facebook_Labels.csv']\n",
    "f2 = ['Final_Dataset_Twitter.csv', 'Final_Dataset_Twitter_Labels.csv']\n",
    "\n",
    "d11, d12 = pd.read_csv(f1[0]), pd.read_csv(f1[1])  # encoding=\"ISO-8859-1\"\n",
    "d21, d22 = pd.read_csv(f1[0]), pd.read_csv(f1[1])\n",
    "\n",
    "df1 = pd.concat([d11, d12], axis=1)\n",
    "df2 = pd.concat([d21, d22], axis=1)\n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "df.drop(columns=['Unnamed: 1'], inplace=True)\n",
    "\n",
    "df.columns = ['tweet', 'label']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanitized = []\n",
    "\n",
    "for tweet in df['tweet']:\n",
    "    tweet = cleanse_links(tweet)\n",
    "    tweet = cleanse_tags_mentions(tweet)\n",
    "    tweet = expand_contractions(tweet)\n",
    "    tweet = spell_correct(tweet)\n",
    "    tweet = remove_stop_words(tweet)\n",
    "    tweet = remove_excess_whitespace(tweet)\n",
    "    sanitized.append(tweet)\n",
    "\n",
    "df['sanitized'] = sanitized\n",
    "\n",
    "df\n",
    "\n",
    "df.to_csv('cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                   tweet  label  \\\n0      Sorry No Comments ..I came here to read commen...      1   \n1      Hi guys. I'm so happy and proud of myself and ...      1   \n2                         Hahahaha your intelligence ğŸ˜œğŸ˜œğŸ˜œ      1   \n3             Aqsa Naveed we were proud backbenchers ğŸ˜œğŸ˜œğŸ˜‚      1   \n4        Hemant extraordinary sketcherğŸ˜..right Ujjawal??      1   \n...                                                  ...    ...   \n10937                        fuck ps3 go get a xbox 360â˜º      0   \n10938            Happy Bithday Windows 7.. I â™¥ Windows 7      0   \n10939                  I can't wait I'm soo excited! ğŸ˜€ğŸ˜€ğŸ˜€      0   \n10940  windows live is very good, I use it more than ...      0   \n10941       I really enjoyed perusing this.Inspiring â™¥â™¥â™¥      0   \n\n                                               sanitized  \n0              Sorry Comments ..I came read comments ..ğŸ˜œ  \n1      Hi guys. happy proud thought share you!!! Toda...  \n2                              Hahahaha intelligence ğŸ˜œğŸ˜œğŸ˜œ  \n3                     Aqsa Naveed proud backbenchers ğŸ˜œğŸ˜œğŸ˜‚  \n4        Hemant extraordinary sketcherğŸ˜..right Ujjawal??  \n...                                                  ...  \n10937                                  fuck ps3 box 360â˜º  \n10938              Happy Bithday Windows 7.. â™¥ Windows 7  \n10939                          not wait soo excited! ğŸ˜€ğŸ˜€ğŸ˜€  \n10940  windows live good, use others. storage nice to...  \n10941                enjoyed perusing this.Inspiring â™¥â™¥â™¥  \n\n[10926 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>label</th>\n      <th>sanitized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sorry No Comments ..I came here to read commen...</td>\n      <td>1</td>\n      <td>Sorry Comments ..I came read comments ..ğŸ˜œ</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Hi guys. I'm so happy and proud of myself and ...</td>\n      <td>1</td>\n      <td>Hi guys. happy proud thought share you!!! Toda...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hahahaha your intelligence ğŸ˜œğŸ˜œğŸ˜œ</td>\n      <td>1</td>\n      <td>Hahahaha intelligence ğŸ˜œğŸ˜œğŸ˜œ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Aqsa Naveed we were proud backbenchers ğŸ˜œğŸ˜œğŸ˜‚</td>\n      <td>1</td>\n      <td>Aqsa Naveed proud backbenchers ğŸ˜œğŸ˜œğŸ˜‚</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hemant extraordinary sketcherğŸ˜..right Ujjawal??</td>\n      <td>1</td>\n      <td>Hemant extraordinary sketcherğŸ˜..right Ujjawal??</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10937</th>\n      <td>fuck ps3 go get a xbox 360â˜º</td>\n      <td>0</td>\n      <td>fuck ps3 box 360â˜º</td>\n    </tr>\n    <tr>\n      <th>10938</th>\n      <td>Happy Bithday Windows 7.. I â™¥ Windows 7</td>\n      <td>0</td>\n      <td>Happy Bithday Windows 7.. â™¥ Windows 7</td>\n    </tr>\n    <tr>\n      <th>10939</th>\n      <td>I can't wait I'm soo excited! ğŸ˜€ğŸ˜€ğŸ˜€</td>\n      <td>0</td>\n      <td>not wait soo excited! ğŸ˜€ğŸ˜€ğŸ˜€</td>\n    </tr>\n    <tr>\n      <th>10940</th>\n      <td>windows live is very good, I use it more than ...</td>\n      <td>0</td>\n      <td>windows live good, use others. storage nice to...</td>\n    </tr>\n    <tr>\n      <th>10941</th>\n      <td>I really enjoyed perusing this.Inspiring â™¥â™¥â™¥</td>\n      <td>0</td>\n      <td>enjoyed perusing this.Inspiring â™¥â™¥â™¥</td>\n    </tr>\n  </tbody>\n</table>\n<p>10926 rows Ã— 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# df = pd.read_csv('cleaned_data.csv')\n",
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "data = [word_tokenize(tweet) for tweet in df.sanitized.values]\n",
    "emoji_list = [[emoji for emoji in extract_emojis(tweet) if char_is_emoji(emoji)] for tweet in df.sanitized.values]\n",
    "\n",
    "model1 = Word2Vec(\n",
    "    data, size=300, window=10, min_count=2, workers=10)\n",
    "model1.train(data, total_examples=len(data), epochs=10)\n",
    "X1 = [sum(model1[j] for j in i) for i in data]\n",
    "\n",
    "model2 = KeyedVectors.load_word2vec_format(\n",
    "\t\t\"emoji2vec.bin\", binary=True)\n",
    "X2 = []\n",
    "for i in emoji_list:\n",
    "\tk = 0\n",
    "\tfor j in i:\n",
    "\t\ttry: k += model2[j]\n",
    "\t\texcept: pass\n",
    "\tX2.append(k)\n",
    "# X2 = [[model2[j] for j in i] for i in emoji_list]\n",
    "\n",
    "X3 = np.array([x1 * x2 for x1, x2 in zip(X1, X2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "X = X3\n",
    "Y = df.label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.9771271729185728\n"
    }
   ],
   "source": [
    "clf1 = SGDClassifier(max_iter=1000, random_state=0, tol=1e-3)\n",
    "clf1.fit(X_train, y_train)\n",
    "print(clf1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.9890210430009149\n"
    }
   ],
   "source": [
    "clf2 =  AdaBoostClassifier(n_estimators=100)\n",
    "clf2.fit(X_train, y_train)\n",
    "print(clf2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.9835315645013724\n"
    }
   ],
   "source": [
    "clf3 = LogisticRegression(multi_class='multinomial')\n",
    "clf3.fit(X_train, y_train)\n",
    "print(clf3.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.9839890210430009\n"
    }
   ],
   "source": [
    "eclf1 = VotingClassifier(estimators=[('sgd', clf1), ('ada', clf2), ('lr', clf3)], voting='hard')\n",
    "eclf1 = eclf1.fit(X_train, y_train)\n",
    "print(eclf1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import ShuffleSplit\n",
    "# n_samples = X.shape[0]\n",
    "# cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "# cross_val_score(clf, X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [word_tokenize(tweet) for tweet in df.sanitized.values]\n",
    "idx = [i for i, tokens in enumerate(data) if len(tokens) > 4]\n",
    "data = [tokens for tokens in data if len(tokens) > 4]\n",
    "data1 = [i[:len(i) // 2] for i in data]\n",
    "data2 = [i[len(i) // 2:] for i in data]\n",
    "\n",
    "model1 = Word2Vec(\n",
    "    data, size=300, window=10, min_count=2, workers=10)\n",
    "model1.train(data1, total_examples=len(data1), epochs=10)\n",
    "X1 = [sum(model1[j] for j in i) for i in data1]\n",
    "\n",
    "model2 = Word2Vec(\n",
    "    data2, size=300, window=10, min_count=2, workers=10)\n",
    "model1.train(data2, total_examples=len(data2), epochs=10)\n",
    "X2 = [sum(model2[j] for j in i) for i in data2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for X in [X1, X2]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=False)\n",
    "\n",
    "    clf = LogisticRegression(multi_class='multinomial')\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds.append(clf.predict(X_test))\n",
    "\n",
    "print(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbasecondae24d4f3be56c48bd9cc7fd53c4875292"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}