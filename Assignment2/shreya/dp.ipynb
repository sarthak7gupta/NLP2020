{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "## Plotly\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "# Others\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import re\n",
    "#nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "#genism\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>@ICICIBank_Care Bank employees of 21 c Faridab...</td>\n",
       "      <td>displeasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@AxisBankSupport 18..233.... is permanently bu...</td>\n",
       "      <td>displeasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@TheOfficialSBI 3 hours in line,still waiting....</td>\n",
       "      <td>displeasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@HDFCBank_Cares Both the ATM machines here in ...</td>\n",
       "      <td>displeasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@ICICIBank Thanks icici bank I got case in you...</td>\n",
       "      <td>miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5689</td>\n",
       "      <td>@HDFCBank_Cares over 25 calls this week.I gues...</td>\n",
       "      <td>displeasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5690</td>\n",
       "      <td>@ICICIBank_Care e-Pay slip as system not worki...</td>\n",
       "      <td>displeasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5691</td>\n",
       "      <td>@TheOfficialSBI staff at Sbi Akaltara branch C...</td>\n",
       "      <td>miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5692</td>\n",
       "      <td>@singhraviranja4 @AxisBankSupport Hi, sorry fo...</td>\n",
       "      <td>miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5693</td>\n",
       "      <td>@TheOfficialSBI @RBI @FinMinIndia an aged man ...</td>\n",
       "      <td>displeasure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5694 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet            tag\n",
       "0     @ICICIBank_Care Bank employees of 21 c Faridab...    displeasure\n",
       "1     @AxisBankSupport 18..233.... is permanently bu...    displeasure\n",
       "2     @TheOfficialSBI 3 hours in line,still waiting....    displeasure\n",
       "3     @HDFCBank_Cares Both the ATM machines here in ...    displeasure\n",
       "4     @ICICIBank Thanks icici bank I got case in you...  miscellaneous\n",
       "...                                                 ...            ...\n",
       "5689  @HDFCBank_Cares over 25 calls this week.I gues...    displeasure\n",
       "5690  @ICICIBank_Care e-Pay slip as system not worki...    displeasure\n",
       "5691  @TheOfficialSBI staff at Sbi Akaltara branch C...  miscellaneous\n",
       "5692  @singhraviranja4 @AxisBankSupport Hi, sorry fo...  miscellaneous\n",
       "5693  @TheOfficialSBI @RBI @FinMinIndia an aged man ...    displeasure\n",
       "\n",
       "[5694 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=['tweet','tag','n']\n",
    "df=pd.read_csv(\"./FinalTweetList.csv\",names=cols)\n",
    "df=df.drop(df.columns[2],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tweet):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(tweet) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    #print(\" \".join(filtered_sentence))\n",
    "    return \" \".join(filtered_sentence)\n",
    "#function for cleaning the tweets\n",
    "def clean(tweet):\n",
    "    tweet=tweet.lower()\n",
    "    tweet=tweet.lstrip()\n",
    "    tweet=tweet.rstrip()\n",
    "    tweet=remove_stopwords(tweet)\n",
    "    negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "    neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], tweet)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in WordPunctTokenizer().tokenize(letters_only) if len(x) > 1]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         displeasure\n",
       "1         displeasure\n",
       "2         displeasure\n",
       "3         displeasure\n",
       "4       miscellaneous\n",
       "            ...      \n",
       "5689      displeasure\n",
       "5690      displeasure\n",
       "5691    miscellaneous\n",
       "5692    miscellaneous\n",
       "5693      displeasure\n",
       "Name: tag, Length: 5694, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'] = df['tweet'].map(lambda x: clean(x))\n",
    "df[\"tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "for label in df[\"tag\"]:\n",
    "    if(label)==\"displeasure\":\n",
    "        labels.append(0)\n",
    "    elif(label==\"miscellaneous\"):\n",
    "        labels.append(1)\n",
    "    elif(label==\"compliment\"):\n",
    "        labels.append(2)\n",
    "    else:\n",
    "        print(\"wrong\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size=0\n",
    "for i in df[\"tweet\"]:\n",
    "    for word in i.split(\" \"):\n",
    "        if word not in l:\n",
    "            vocabulary_size=vocabulary_size+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create sequence\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(df['tweet'])\n",
    "sequences = tokenizer.texts_to_sequences(df['tweet'])\n",
    "data = pad_sequences(sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning:\n",
      "\n",
      "Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5204 samples, validate on 490 samples\n",
      "Epoch 1/3\n",
      "5204/5204 [==============================] - 14s 3ms/step - loss: 0.4713 - accuracy: 0.4800 - val_loss: -0.2663 - val_accuracy: 0.5918\n",
      "Epoch 2/3\n",
      "5204/5204 [==============================] - 13s 2ms/step - loss: -0.4728 - accuracy: 0.5790 - val_loss: -1.2853 - val_accuracy: 0.5286\n",
      "Epoch 3/3\n",
      "5204/5204 [==============================] - 12s 2ms/step - loss: -1.4689 - accuracy: 0.5742 - val_loss: -2.1866 - val_accuracy: 0.5265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a4ce29fd0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Network architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(l), 100, input_length=50))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## Fit the model\n",
    "model.fit(data, np.array(labels), validation_split=0.086, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-a9ce8643ae52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0878\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "X = df.tweet\n",
    "y = df.tag\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.0878, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('embedded.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 797 samples, validate on 4897 samples\n",
      "Epoch 1/3\n",
      "797/797 [==============================] - 2s 3ms/step - loss: 0.6893 - accuracy: 0.4705 - val_loss: 0.6552 - val_accuracy: 0.3923\n",
      "Epoch 2/3\n",
      "797/797 [==============================] - 1s 2ms/step - loss: 0.6496 - accuracy: 0.4843 - val_loss: 0.6173 - val_accuracy: 0.4789\n",
      "Epoch 3/3\n",
      "797/797 [==============================] - 1s 2ms/step - loss: 0.5706 - accuracy: 0.5721 - val_loss: 0.5454 - val_accuracy: 0.4919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a4f799f60>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create model\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(vocabulary_size, 100, input_length=50, weights=[embedding_matrix], trainable=False))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(100))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## Fit train data\n",
    "model_glove.fit(data, np.array(labels), validation_split=0.86, epochs = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, None, 100)         51300     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 116,729\n",
      "Trainable params: 65,429\n",
      "Non-trainable params: 51,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocabulary_size, embedding_dim, weights=[embedding_matrix], trainable=False))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 797 samples, validate on 4897 samples\n",
      "Epoch 1/3\n",
      "797/797 [==============================] - 1s 2ms/step - loss: 0.5410 - accuracy: 0.5659 - val_loss: 0.5126 - val_accuracy: 0.5289\n",
      "Epoch 2/3\n",
      "797/797 [==============================] - 1s 2ms/step - loss: 0.4319 - accuracy: 0.6198 - val_loss: 0.5064 - val_accuracy: 0.4864\n",
      "Epoch 3/3\n",
      "797/797 [==============================] - 1s 2ms/step - loss: 0.3566 - accuracy: 0.6136 - val_loss: 0.4853 - val_accuracy: 0.5542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a4cf34f28>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove.fit(data, np.array(labels), validation_split=0.86, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-45799fb1d7a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_x' is not defined"
     ]
    }
   ],
   "source": [
    "model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
