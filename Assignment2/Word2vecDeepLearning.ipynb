{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import keras\n",
    "from keras import utils\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import multiprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "from operator import getitem\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'up', 'down', 'these', 'and', 'about', 'him', 'we', 'himself', 'to', 'if', 'where', 'only', \"didn't\", 'how', \"she's\", 'whom', 'an', 'ourselves', 'myself', 'but', 'your', \"should've\", 'what', 'few', 'do', 'have', 'each', 'themselves', 'or', 'm', \"you'd\", 'on', 'because', 'there', 'will', 'me', 'it', 'yourselves', 's', 'too', 'before', 'own', 'from', 'should', 'my', 'can', 'am', 'doing', 'is', 'further', 'until', 'which', 'into', 'again', 'such', 'ma', 'you', 'yours', 'he', 'does', 'very', 'just', 'out', 'a', 'she', 'his', 'who', 'while', 'are', 'over', 're', 'they', 'other', 'at', 'same', 'did', 'some', 'having', 'of', 'more', 'then', 'theirs', 'y', 'most', 'as', 'during', 'here', 'in', 've', 'herself', 'now', 'this', 'being', 'yourself', 'has', 'hers', 'for', 'll', 'any', 'i', 'itself', \"you'll\", 't', \"it's\", 'their', 'were', 'with', 'that', \"you're\", 'through', \"you've\", 'after', 'so', \"that'll\", 'o', 'why', 'than', 'off', 'ours', 'both', 'once', 'them', 'd', 'been', 'our', 'all', 'those', 'above', 'by', 'between', 'under', 'when', 'the', 'had', 'its', 'was', 'her', 'be', 'below'}\n"
     ]
    }
   ],
   "source": [
    "stopwords_set = set(stopwords.words('english'))\n",
    "required = {\"needn\", \"doesn\", \"won\", \"shan't\", \"don\", \"ain\", \"not\", \"didn\", \"hadn\", \"haven't\", \"couldn't\", \"wasn't\", \"aren't\", \"isn\", \"needn't\", \"aren\", \"wouldn\", \"shouldn\", \"hasn't\", \"shan\", \"no\", \"wasn\", \"nor\", \"hasn\", \"mightn\", \"doesn't\", \"against\", \"wouldn't\", \"couldn\", \"hadn't\", \"isn't\", \"mustn\", \"don't\", \"weren't\", \"haven\", \"mustn't\", \"shouldn't\", \"weren\", \"won't\", \"mightn't\"}\n",
    "stopwords_set -= required\n",
    "print(stopwords_set)\n",
    "dictionary = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"FinalTweetList_train.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncompliment = df[df[1]=='compliment']\\ndispleasure = df[df[1]=='displeasure']\\nmiscellaneous = df[df[1]=='miscellaneous']\\nsize = min(len(displeasure), len(miscellaneous))\\nprint(size)\\ncompliment_extra = pd.DataFrame()\\nwhile(len(compliment) + len(compliment_extra) < size):\\n    compliment_extra = compliment_extra.append(compliment.sample(min(size - (len(compliment) + len(compliment_extra)),len(compliment))))\\ncompliment = compliment_extra.append(compliment)\\n\\ndf = compliment.append([displeasure, miscellaneous])\\ndf = df.reset_index(drop = True)\\nprint(df)\\n\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#upsample compliments to equalise classes\n",
    "'''\n",
    "compliment = df[df[1]=='compliment']\n",
    "displeasure = df[df[1]=='displeasure']\n",
    "miscellaneous = df[df[1]=='miscellaneous']\n",
    "size = min(len(displeasure), len(miscellaneous))\n",
    "print(size)\n",
    "compliment_extra = pd.DataFrame()\n",
    "while(len(compliment) + len(compliment_extra) < size):\n",
    "    compliment_extra = compliment_extra.append(compliment.sample(min(size - (len(compliment) + len(compliment_extra)),len(compliment))))\n",
    "compliment = compliment_extra.append(compliment)\n",
    "\n",
    "df = compliment.append([displeasure, miscellaneous])\n",
    "df = df.reset_index(drop = True)\n",
    "print(df)\n",
    "'''\n",
    "# partially stratified upsample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       0              1\n",
      "0      at_user at_user at_user at_user at_user at_use...     compliment\n",
      "1      rt at_user at_user at_user at_user at_user at_...  miscellaneous\n",
      "2      first hand experience of the cashrush process ...     compliment\n",
      "3      at_user at_user hi, sorry for the trouble. we'...  miscellaneous\n",
      "4               at_user at_user the link is not opening.  miscellaneous\n",
      "...                                                  ...            ...\n",
      "11374  at_user in queue since 7 am at_user out of cur...  miscellaneous\n",
      "11375  at_user at_user at_user at_user at_user at_use...  miscellaneous\n",
      "11376  thank you bankers for your kind co operation. ...     compliment\n",
      "11377  hell lot of sms by at_user at_user but no atms...     compliment\n",
      "11378  at_user at_user at_user at_user atps not worki...    displeasure\n",
      "\n",
      "[11379 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def processTweet(tweet):\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    return tweet\n",
    "\n",
    "for i in range(len(df[0])):\n",
    "    df[0][i] = processTweet(df[0][i])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11379\n"
     ]
    }
   ],
   "source": [
    "#Stopword removal, non-alphabet tokens removed, optional non-english word removal\n",
    "\n",
    "#train, test = train_test_split(df, test_size = (500 if len(feats)>500 else 0.25), stratify = df[1], random_state = 21)\n",
    "#train, test = train_test_split(df, test_size = (500 if len(feats)>500 else 0.25), random_state = 21)\n",
    "#train = train.reset_index(drop = True)\n",
    "#test = test.reset_index(drop = True)\n",
    "train = df\n",
    "\n",
    "def word_clean(words):\n",
    "    #return ' '.join([word for word in words if (word.isalpha())])\n",
    "    #return ' '.join([word for word in words if (word.isalpha() and word in dictionary)])\n",
    "    return ' '.join([word for word in words if (word.isalpha() and word not in stopwords_set)])\n",
    "    #return ' '.join([word for word in words if (word.isalpha() and word not in stopwords_set and word in dictionary)])\n",
    "    \n",
    "def clean_tweets(dataframe):\n",
    "    return [word_clean(WordPunctTokenizer().tokenize(dataframe[0][i])) for i in range(len(dataframe[0]))]\n",
    "\n",
    "x_train = clean_tweets(train)\n",
    "y_train = train[1]\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 6985\n",
      "Word2Vec trained\n",
      "LabelEncoder\n",
      "Fit Tokenizer\n",
      "Number of unique words: 6986\n",
      "Create Embedding matrix\n",
      "Embedding matrix: (6986, 300)\n",
      "Build Keras model\n",
      "x_train shape: (11379, 500)\n",
      "y_train shape: (11379, 3)\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 300)          2095800   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 2,342,395\n",
      "Trainable params: 246,595\n",
      "Non-trainable params: 2,095,800\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fit Keras model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:998: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "11379/11379 [==============================] - 300s 26ms/step - loss: 0.9053 - accuracy: 0.5279\n",
      "Epoch 2/32\n",
      "11379/11379 [==============================] - 289s 25ms/step - loss: 0.7343 - accuracy: 0.6740\n",
      "Epoch 3/32\n",
      "11379/11379 [==============================] - 294s 26ms/step - loss: 0.6487 - accuracy: 0.7247\n",
      "Epoch 4/32\n",
      "11379/11379 [==============================] - 298s 26ms/step - loss: 0.5881 - accuracy: 0.7507\n",
      "Epoch 5/32\n",
      "11379/11379 [==============================] - 306s 27ms/step - loss: 0.5361 - accuracy: 0.7809\n",
      "Epoch 6/32\n",
      "11379/11379 [==============================] - 307s 27ms/step - loss: 0.4837 - accuracy: 0.8047\n",
      "Epoch 7/32\n",
      "11379/11379 [==============================] - 305s 27ms/step - loss: 0.4449 - accuracy: 0.8226\n",
      "Epoch 8/32\n",
      "11379/11379 [==============================] - 308s 27ms/step - loss: 0.4055 - accuracy: 0.8414\n",
      "Epoch 9/32\n",
      "11379/11379 [==============================] - 307s 27ms/step - loss: 0.3684 - accuracy: 0.8591\n",
      "Epoch 10/32\n",
      "11379/11379 [==============================] - 307s 27ms/step - loss: 0.3397 - accuracy: 0.8698\n",
      "Epoch 11/32\n",
      "11379/11379 [==============================] - 305s 27ms/step - loss: 0.3133 - accuracy: 0.8810\n",
      "Epoch 12/32\n",
      "11379/11379 [==============================] - 307s 27ms/step - loss: 0.2984 - accuracy: 0.8874\n",
      "Epoch 13/32\n",
      "11379/11379 [==============================] - 305s 27ms/step - loss: 0.2822 - accuracy: 0.8938\n",
      "Epoch 14/32\n",
      "11379/11379 [==============================] - 306s 27ms/step - loss: 0.2647 - accuracy: 0.8992\n",
      "Epoch 15/32\n",
      "11379/11379 [==============================] - 306s 27ms/step - loss: 0.2455 - accuracy: 0.9097\n",
      "Epoch 16/32\n",
      "11379/11379 [==============================] - 308s 27ms/step - loss: 0.2346 - accuracy: 0.9154\n",
      "Epoch 17/32\n",
      "11379/11379 [==============================] - 305s 27ms/step - loss: 0.2103 - accuracy: 0.9240\n",
      "Epoch 18/32\n",
      "11379/11379 [==============================] - 308s 27ms/step - loss: 0.2045 - accuracy: 0.9252\n",
      "Epoch 19/32\n",
      "11379/11379 [==============================] - 307s 27ms/step - loss: 0.1976 - accuracy: 0.9251\n",
      "Epoch 20/32\n",
      "11379/11379 [==============================] - 308s 27ms/step - loss: 0.1852 - accuracy: 0.9324\n",
      "Epoch 21/32\n",
      "11379/11379 [==============================] - 308s 27ms/step - loss: 0.1797 - accuracy: 0.9341\n",
      "Epoch 22/32\n",
      "11379/11379 [==============================] - 307s 27ms/step - loss: 0.1660 - accuracy: 0.9384\n",
      "Epoch 23/32\n",
      "11379/11379 [==============================] - 308s 27ms/step - loss: 0.1704 - accuracy: 0.9354\n",
      "Epoch 24/32\n",
      "11379/11379 [==============================] - 306s 27ms/step - loss: 0.1704 - accuracy: 0.9374\n",
      "Epoch 25/32\n",
      "11379/11379 [==============================] - 308s 27ms/step - loss: 0.1513 - accuracy: 0.9435\n",
      "Epoch 26/32\n",
      "11379/11379 [==============================] - 307s 27ms/step - loss: 0.1572 - accuracy: 0.9426\n",
      "Epoch 27/32\n",
      "11379/11379 [==============================] - 307s 27ms/step - loss: 0.1491 - accuracy: 0.9468\n",
      "Epoch 28/32\n",
      "11379/11379 [==============================] - 311s 27ms/step - loss: 0.1339 - accuracy: 0.9497\n",
      "Epoch 29/32\n",
      "11379/11379 [==============================] - 307s 27ms/step - loss: 0.1297 - accuracy: 0.9521\n",
      "Epoch 30/32\n",
      "11379/11379 [==============================] - 305s 27ms/step - loss: 0.1328 - accuracy: 0.9496\n",
      "Epoch 31/32\n",
      "11379/11379 [==============================] - 334s 29ms/step - loss: 0.1292 - accuracy: 0.9530\n",
      "Epoch 32/32\n",
      "11379/11379 [==============================] - 380s 33ms/step - loss: 0.1295 - accuracy: 0.9503\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "x_train = [gensim.utils.simple_preprocess(text) for text in x_train]\n",
    "#Continous bag of words used\n",
    "w2v_model = gensim.models.Word2Vec(min_count=1, window=5,\n",
    "                                        size=300,\n",
    "                                        workers=multiprocessing.cpu_count())\n",
    "w2v_model.build_vocab(x_train)\n",
    "w2v_model.train(x_train, total_examples=w2v_model.corpus_count, epochs=100)\n",
    "w2v_words = list(w2v_model.wv.vocab)\n",
    "print(\"Vocabulary size: %i\" % len(w2v_words))\n",
    "print(\"Word2Vec trained\")\n",
    "\n",
    "print(\"LabelEncoder\")\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "print(\"Fit Tokenizer\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(x_train),\n",
    "                                                     maxlen=500)\n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "print(\"Number of unique words: %i\" % num_words)\n",
    "\n",
    "print(\"Create Embedding matrix\")\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, idx in word_index.items():\n",
    "    if word in w2v_words:\n",
    "        embedding_vector = w2v_model.wv.get_vector(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = w2v_model.wv[word]\n",
    "print(\"Embedding matrix: %s\" % str(embedding_matrix.shape))\n",
    "\n",
    "print(\"Build Keras model\")\n",
    "print('x_train shape: %s' % str(x_train.shape))\n",
    "print('y_train shape: %s' % str(y_train.shape))\n",
    "\n",
    "k_model = Sequential()\n",
    "k_model.add(Embedding(vocab_size,\n",
    "                           300,\n",
    "                           weights=[embedding_matrix],\n",
    "                           input_length=500,\n",
    "                           trainable=False))\n",
    "k_model.add(LSTM(128, dropout=0.5, recurrent_dropout=0.2))\n",
    "for hidden_layer in (128, 64, 32):\n",
    "    k_model.add(Dense(hidden_layer, activation='relu'))\n",
    "    k_model.add(Dropout(0.2))\n",
    "if num_classes > 2:\n",
    "    k_model.add(Dense(num_classes, activation='softmax'))\n",
    "else:\n",
    "    k_model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "k_model.compile(loss='categorical_crossentropy' if num_classes > 2 else 'binary_crossentropy',\n",
    "                     optimizer='adam',\n",
    "                     metrics=['accuracy'])\n",
    "print(k_model.summary())\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='accuracy', patience=6, verbose=0, mode='max')\n",
    "rop = ReduceLROnPlateau(monitor='accuracy', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='max')\n",
    "callbacks = [early_stopping, rop]\n",
    "\n",
    "print(\"Fit Keras model\")\n",
    "k_model.fit(x_train, y_train,\n",
    "                 batch_size=128,\n",
    "                 epochs=32,\n",
    "                 callbacks=callbacks,\n",
    "                 verbose=1)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Kotak Bank:  38.70% in  500 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       0.90      0.92      0.91       113\n",
      "  displeasure       0.91      0.90      0.91       232\n",
      "miscellaneous       0.86      0.86      0.86       155\n",
      "\n",
      "     accuracy                           0.89       500\n",
      "    macro avg       0.89      0.90      0.89       500\n",
      " weighted avg       0.89      0.89      0.89       500\n",
      "\n",
      "                             Axis Bank:  34.80% in  500 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       0.87      1.00      0.93        41\n",
      "  displeasure       0.99      0.99      0.99       201\n",
      "miscellaneous       0.99      0.97      0.98       258\n",
      "\n",
      "     accuracy                           0.98       500\n",
      "    macro avg       0.95      0.99      0.97       500\n",
      " weighted avg       0.98      0.98      0.98       500\n",
      "\n",
      "                             HDFC Bank:  32.90% in  500 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       0.95      0.97      0.96        61\n",
      "  displeasure       0.96      0.96      0.96       234\n",
      "miscellaneous       0.96      0.96      0.96       205\n",
      "\n",
      "     accuracy                           0.96       500\n",
      "    macro avg       0.96      0.96      0.96       500\n",
      " weighted avg       0.96      0.96      0.96       500\n",
      "\n",
      "                   State Bank of India:  29.20% in  500 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       0.69      0.62      0.65        55\n",
      "  displeasure       0.76      0.82      0.79       238\n",
      "miscellaneous       0.78      0.73      0.75       207\n",
      "\n",
      "     accuracy                           0.76       500\n",
      "    macro avg       0.74      0.72      0.73       500\n",
      " weighted avg       0.76      0.76      0.76       500\n",
      "\n",
      "                            ICICI Bank:  28.90% in  500 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       1.00      1.00      1.00        65\n",
      "  displeasure       1.00      0.99      0.99       278\n",
      "miscellaneous       0.98      0.99      0.99       157\n",
      "\n",
      "     accuracy                           0.99       500\n",
      "    macro avg       0.99      0.99      0.99       500\n",
      " weighted avg       0.99      0.99      0.99       500\n",
      "\n",
      "                 Reserve Bank of India:  27.47% in  253 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       1.00      1.00      1.00        22\n",
      "  displeasure       0.96      0.96      0.96       136\n",
      "miscellaneous       0.95      0.95      0.95        95\n",
      "\n",
      "     accuracy                           0.96       253\n",
      "    macro avg       0.97      0.97      0.97       253\n",
      " weighted avg       0.96      0.96      0.96       253\n",
      "\n",
      "                        Bank of Baroda:  35.51% in  138 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       0.78      0.78      0.78        23\n",
      "  displeasure       0.68      0.77      0.72        56\n",
      "miscellaneous       0.77      0.68      0.72        59\n",
      "\n",
      "     accuracy                           0.73       138\n",
      "    macro avg       0.74      0.74      0.74       138\n",
      " weighted avg       0.74      0.73      0.73       138\n",
      "\n",
      "                  Punjab National Bank:  42.11% in   19 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       1.00      1.00      1.00         4\n",
      "  displeasure       1.00      1.00      1.00         7\n",
      "miscellaneous       1.00      1.00      1.00         8\n",
      "\n",
      "     accuracy                           1.00        19\n",
      "    macro avg       1.00      1.00      1.00        19\n",
      " weighted avg       1.00      1.00      1.00        19\n",
      "\n",
      "                              Yes Bank:  42.11% in   19 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       1.00      1.00      1.00         4\n",
      "  displeasure       1.00      1.00      1.00         7\n",
      "miscellaneous       1.00      1.00      1.00         8\n",
      "\n",
      "     accuracy                           1.00        19\n",
      "    macro avg       1.00      1.00      1.00        19\n",
      " weighted avg       1.00      1.00      1.00        19\n",
      "\n",
      "                         IndusInd Bank:  34.62% in   13 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       1.00      1.00      1.00         1\n",
      "  displeasure       1.00      1.00      1.00         5\n",
      "miscellaneous       1.00      1.00      1.00         7\n",
      "\n",
      "     accuracy                           1.00        13\n",
      "    macro avg       1.00      1.00      1.00        13\n",
      " weighted avg       1.00      1.00      1.00        13\n",
      "\n",
      "                             IDBI Bank:  45.45% in   11 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       1.00      1.00      1.00         3\n",
      "  displeasure       1.00      1.00      1.00         4\n",
      "miscellaneous       1.00      1.00      1.00         4\n",
      "\n",
      "     accuracy                           1.00        11\n",
      "    macro avg       1.00      1.00      1.00        11\n",
      " weighted avg       1.00      1.00      1.00        11\n",
      "\n",
      "                              Citibank:  18.18% in   11 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       1.00      1.00      1.00         1\n",
      "  displeasure       1.00      1.00      1.00         8\n",
      "miscellaneous       1.00      1.00      1.00         2\n",
      "\n",
      "     accuracy                           1.00        11\n",
      "    macro avg       1.00      1.00      1.00        11\n",
      " weighted avg       1.00      1.00      1.00        11\n",
      "\n",
      "                         Bank of India:  25.00% in    6 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  displeasure       1.00      1.00      1.00         3\n",
      "miscellaneous       1.00      1.00      1.00         3\n",
      "\n",
      "     accuracy                           1.00         6\n",
      "    macro avg       1.00      1.00      1.00         6\n",
      " weighted avg       1.00      1.00      1.00         6\n",
      "\n",
      "                                 PayTM:  40.00% in    5 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       1.00      1.00      1.00         1\n",
      "  displeasure       1.00      1.00      1.00         2\n",
      "miscellaneous       1.00      1.00      1.00         2\n",
      "\n",
      "     accuracy                           1.00         5\n",
      "    macro avg       1.00      1.00      1.00         5\n",
      " weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "                           Vijaya Bank:  50.00% in    4 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       1.00      1.00      1.00         1\n",
      "  displeasure       1.00      1.00      1.00         1\n",
      "miscellaneous       1.00      1.00      1.00         2\n",
      "\n",
      "     accuracy                           1.00         4\n",
      "    macro avg       1.00      1.00      1.00         4\n",
      " weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "                           Canara Bank:  66.67% in    3 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       1.00      1.00      1.00         1\n",
      "miscellaneous       1.00      1.00      1.00         2\n",
      "\n",
      "     accuracy                           1.00         3\n",
      "    macro avg       1.00      1.00      1.00         3\n",
      " weighted avg       1.00      1.00      1.00         3\n",
      "\n",
      "                      Corporation Bank:  33.33% in    3 tweets\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  compliment       1.00      1.00      1.00         1\n",
      " displeasure       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         3\n",
      "   macro avg       1.00      1.00      1.00         3\n",
      "weighted avg       1.00      1.00      1.00         3\n",
      "\n",
      "                           Post Office:  33.33% in    3 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  displeasure       1.00      1.00      1.00         1\n",
      "miscellaneous       1.00      1.00      1.00         2\n",
      "\n",
      "     accuracy                           1.00         3\n",
      "    macro avg       1.00      1.00      1.00         3\n",
      " weighted avg       1.00      1.00      1.00         3\n",
      "\n",
      "                   Union Bank of India:  16.67% in    3 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  displeasure       1.00      1.00      1.00         2\n",
      "miscellaneous       1.00      1.00      1.00         1\n",
      "\n",
      "     accuracy                           1.00         3\n",
      "    macro avg       1.00      1.00      1.00         3\n",
      " weighted avg       1.00      1.00      1.00         3\n",
      "\n",
      "                        Allahabad Bank:  50.00% in    2 tweets\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  compliment       1.00      1.00      1.00         1\n",
      " displeasure       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "                          Federal Bank:  50.00% in    2 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "miscellaneous       1.00      1.00      1.00         2\n",
      "\n",
      "     accuracy                           1.00         2\n",
      "    macro avg       1.00      1.00      1.00         2\n",
      " weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "                                  HSBC:  50.00% in    2 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "miscellaneous       1.00      1.00      1.00         2\n",
      "\n",
      "     accuracy                           1.00         2\n",
      "    macro avg       1.00      1.00      1.00         2\n",
      " weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "National Payments Corporation of India:  50.00% in    2 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "miscellaneous       1.00      1.00      1.00         2\n",
      "\n",
      "     accuracy                           1.00         2\n",
      "    macro avg       1.00      1.00      1.00         2\n",
      " weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "                         Oriental Bank:  50.00% in    2 tweets\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  compliment       1.00      1.00      1.00         1\n",
      " displeasure       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "                     Banking Ombudsman:  25.00% in    2 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  displeasure       1.00      1.00      1.00         1\n",
      "miscellaneous       1.00      1.00      1.00         1\n",
      "\n",
      "     accuracy                           1.00         2\n",
      "    macro avg       1.00      1.00      1.00         2\n",
      " weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "                              UCO Bank:  25.00% in    2 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  displeasure       1.00      1.00      1.00         1\n",
      "miscellaneous       1.00      1.00      1.00         1\n",
      "\n",
      "     accuracy                           1.00         2\n",
      "    macro avg       1.00      1.00      1.00         2\n",
      " weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "                          Central Bank:   0.00% in    2 tweets\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " displeasure       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "                            World Bank: 100.00% in    1 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       0.00      0.00      0.00       0.0\n",
      "miscellaneous       0.00      0.00      0.00       1.0\n",
      "\n",
      "     accuracy                           0.00       1.0\n",
      "    macro avg       0.00      0.00      0.00       1.0\n",
      " weighted avg       0.00      0.00      0.00       1.0\n",
      "\n",
      "                       IDFC First Bank:  50.00% in    1 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "miscellaneous       1.00      1.00      1.00         1\n",
      "\n",
      "     accuracy                           1.00         1\n",
      "    macro avg       1.00      1.00      1.00         1\n",
      " weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "                  Indian Overseas Bank:  50.00% in    1 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "miscellaneous       1.00      1.00      1.00         1\n",
      "\n",
      "     accuracy                           1.00         1\n",
      "    macro avg       1.00      1.00      1.00         1\n",
      " weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "                              MobiKwik:  50.00% in    1 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "miscellaneous       1.00      1.00      1.00         1\n",
      "\n",
      "     accuracy                           1.00         1\n",
      "    macro avg       1.00      1.00      1.00         1\n",
      " weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "                   Bank of Maharashtra:   0.00% in    1 tweets\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " displeasure       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "                              DCB Bank:   0.00% in    1 tweets\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " displeasure       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare test\n",
    "\n",
    "values = {\n",
    "    \"compliment\": 1,\n",
    "    \"miscellaneous\": 0.5,\n",
    "    \"displeasure\": 0\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "direc = os.fsencode('./test')\n",
    "for File in os.listdir(direc):\n",
    "    filename = os.fsdecode(File)\n",
    "    try:\n",
    "        test = pd.read_csv(\"./test/\" + filename, header = None)\n",
    "        for i in range(len(test[0])):\n",
    "            test[0][i] = processTweet(test[0][i])\n",
    "        x_test = clean_tweets(test)\n",
    "        y_test = test[1]\n",
    "\n",
    "        x_test = [gensim.utils.simple_preprocess(text) for text in x_test]\n",
    "        x_test = keras.preprocessing.sequence.pad_sequences(\n",
    "            tokenizer.texts_to_sequences(x_test),\n",
    "            maxlen=500)\n",
    "\n",
    "        # Predict\n",
    "        confidences = k_model.predict(x_test, verbose=0)\n",
    "\n",
    "        y_pred_1d = []\n",
    "\n",
    "        for confidence in confidences:\n",
    "            idx = np.argmax(confidence)\n",
    "            y_pred_1d.append(label_encoder.classes_[idx])\n",
    "            \n",
    "        sum = 0\n",
    "        for i in y_pred_1d:\n",
    "            sum += values[i]\n",
    "        result = {}\n",
    "        result['performance_index'] = sum * 100 / len(y_pred_1d)\n",
    "        result['classification_report'] = classification_report(y_test, y_pred_1d)\n",
    "        result['tweets'] = len(y_pred_1d)\n",
    "        results[filename[:-4]] = result\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "for i in sorted(results.items(), key = lambda x: ( -(getitem(x[1],'tweets')), -(getitem(x[1],'performance_index')) )):\n",
    "    print((i[0] + \": \").rjust(40) + (\"{:.2f}\".format(i[1]['performance_index'])).rjust(6) + \"% in \" + str(i[1]['tweets']).rjust(4) + \" tweets\")\n",
    "    print(i[1]['classification_report'])\n",
    "    \n",
    "#The toppers are:\n",
    "    #1. Kotak Bank: 38.70%\n",
    "    #2. Axis Bank: 34.80%\n",
    "    #3. HDFC Bank: 32.90%\n",
    "    #4. State Bank of India: 29.20%\n",
    "    #5. ICICI Bank: 28.90%\n",
    "#Absolutely correct order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
