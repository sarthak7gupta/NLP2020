{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import keras\n",
    "from keras import utils\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import multiprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "from operator import getitem\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yours', 'should', 'while', 'such', 'doing', 'yourself', 'can', 'that', 'she', 'was', 'than', 'its', 'during', 'there', 'them', 't', 'ma', 'his', 'a', 'these', 'herself', 'other', 'again', 'about', 'which', 'each', 'once', \"you'll\", 'is', 'our', 'and', 'under', 'ours', 'most', 'some', 'whom', 'themselves', 'why', 'were', 'it', 'you', 'when', 'out', 'own', 'what', 'to', 'too', 'same', 'those', 'of', 'my', 'few', 'further', 'himself', 'hers', 'having', 'any', 'just', 'do', 'him', 'so', 'over', 'where', 'be', 'both', \"that'll\", 'above', 'had', \"it's\", 'did', 'will', 'being', 'before', 'your', 'm', 'y', 'only', 'now', 'off', 'an', 'from', 's', 'has', 'here', 'for', 'with', 'down', 'in', 'more', 'because', 'have', 'if', 'ourselves', 'all', 'the', \"you'd\", 'after', 'between', 'd', 'll', 'we', 'are', 'but', 'i', 'who', 'o', 'until', 'me', 'theirs', 'through', 'below', 've', 'this', 'myself', 'yourselves', 'how', 'he', 'am', \"should've\", \"you've\", \"didn't\", 'into', 'her', \"she's\", 'their', 'at', \"you're\", 're', 'up', 'itself', 'then', 'been', 'as', 'does', 'on', 'very', 'by', 'they', 'or'}\n"
     ]
    }
   ],
   "source": [
    "stopwords_set = set(stopwords.words('english'))\n",
    "required = {\"needn\", \"doesn\", \"won\", \"shan't\", \"don\", \"ain\", \"not\", \"didn\", \"hadn\", \"haven't\", \"couldn't\", \"wasn't\", \"aren't\", \"isn\", \"needn't\", \"aren\", \"wouldn\", \"shouldn\", \"hasn't\", \"shan\", \"no\", \"wasn\", \"nor\", \"hasn\", \"mightn\", \"doesn't\", \"against\", \"wouldn't\", \"couldn\", \"hadn't\", \"isn't\", \"mustn\", \"don't\", \"weren't\", \"haven\", \"mustn't\", \"shouldn't\", \"weren\", \"won't\", \"mightn't\"}\n",
    "stopwords_set -= required\n",
    "print(stopwords_set)\n",
    "dictionary = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"FinalTweetList_train.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      0              1\n",
      "0     @TheOfficialSBI  @Indiapnb @IDBI_Bank @AxisBan...     compliment\n",
      "1     First hand experience of the #CashRush process...     compliment\n",
      "2     @ksmkkbookscom @HDFC_Bank yeah @AxisBank @Axis...     compliment\n",
      "3     @narendramodi much of the issues will be sorte...     compliment\n",
      "4     @AxisBankSupport Thank u for ur response right...     compliment\n",
      "...                                                 ...            ...\n",
      "4135  #Bankdeposits will spike: How Rs 500, Rs 1000 ...  miscellaneous\n",
      "4136  @ICICIBank_Care got a call from +919821294665 ...  miscellaneous\n",
      "4137  @keshri_niraj @AxisBankSupport Hi, could you p...  miscellaneous\n",
      "4138  Nw banker will undrstnd wht a softwaredevolope...  miscellaneous\n",
      "4139  @TheOfficialSBI @SBICard_Connect I have blocke...  miscellaneous\n",
      "\n",
      "[4140 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#downsample displeasure and miscellaneous to equalise classes\n",
    "\n",
    "\n",
    "compliment = df[df[1]=='compliment']\n",
    "displeasure = df[df[1]=='displeasure']\n",
    "miscellaneous = df[df[1]=='miscellaneous']\n",
    "_, displeasure = train_test_split(displeasure, test_size = len(compliment), random_state = 21)\n",
    "_, miscellaneous = train_test_split(miscellaneous, test_size = len(compliment), random_state = 21)\n",
    "df = compliment.append([displeasure, miscellaneous])\n",
    "df = df.reset_index(drop = True)\n",
    "print(df)\n",
    "\n",
    "#stratified downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      0              1\n",
      "0     at_user at_user at_user at_user at_user at_use...     compliment\n",
      "1     first hand experience of the cashrush process ...     compliment\n",
      "2     at_user at_user yeah at_user at_user r the mos...     compliment\n",
      "3     at_user much of the issues will be sorted if b...     compliment\n",
      "4     at_user thank u for ur response right now. thi...     compliment\n",
      "...                                                 ...            ...\n",
      "4135  bankdeposits will spike: how rs 500, rs 1000 n...  miscellaneous\n",
      "4136  at_user got a call from +919821294665 some ici...  miscellaneous\n",
      "4137  at_user at_user hi, could you please elaborate...  miscellaneous\n",
      "4138  nw banker will undrstnd wht a softwaredevolope...  miscellaneous\n",
      "4139  at_user at_user i have blocked my debit card f...  miscellaneous\n",
      "\n",
      "[4140 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def processTweet(tweet):\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    return tweet\n",
    "\n",
    "for i in range(len(df[0])):\n",
    "    df[0][i] = processTweet(df[0][i])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4140\n"
     ]
    }
   ],
   "source": [
    "train = df\n",
    "\n",
    "def word_clean(words):\n",
    "    #return ' '.join([word for word in words if (word.isalpha())])\n",
    "    #return ' '.join([word for word in words if (word.isalpha() and word in dictionary)])\n",
    "    return ' '.join([word for word in words if (word.isalpha() and word not in stopwords_set)])\n",
    "    #return ' '.join([word for word in words if (word.isalpha() and word not in stopwords_set and word in dictionary)])\n",
    "    \n",
    "def clean_tweets(dataframe):\n",
    "    return [word_clean(WordPunctTokenizer().tokenize(dataframe[0][i])) for i in range(len(dataframe[0]))]\n",
    "\n",
    "x_train = clean_tweets(train)\n",
    "y_train = train[1]\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec trained\n"
     ]
    }
   ],
   "source": [
    "x_train = [gensim.utils.simple_preprocess(text) for text in x_train]\n",
    "#Continous bag of words used\n",
    "w2v_model = gensim.models.Word2Vec(min_count=1, window=5,\n",
    "                                        size=100,\n",
    "                                        workers=multiprocessing.cpu_count())\n",
    "w2v_model.build_vocab(x_train)\n",
    "w2v_model.train(x_train, total_examples=w2v_model.corpus_count, epochs=100)\n",
    "print(\"Word2Vec trained\")\n",
    "\n",
    "x_train = [' '.join(i) for i in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feats(words):\n",
    "    return {tuple(w2v_model.wv[word].tolist()): True for word in words}    #downsample 88.5% train 67.4% test    upsample 80.5% train 74% test\n",
    "    \n",
    "def extract_feats(xdataframe, ydataframe):\n",
    "    return [(word_feats(WordPunctTokenizer().tokenize(xdataframe[i])), ydataframe[i]) for i in range(len(xdataframe))]\n",
    "\n",
    "train_set = extract_feats(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (25 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.333\n",
      "             2          -1.09783        0.869\n",
      "             3          -1.09705        0.869\n",
      "             4          -1.09626        0.869\n",
      "             5          -1.09548        0.869\n",
      "             6          -1.09470        0.869\n",
      "             7          -1.09392        0.869\n",
      "             8          -1.09314        0.869\n",
      "             9          -1.09237        0.869\n",
      "            10          -1.09159        0.869\n",
      "            11          -1.09081        0.869\n",
      "            12          -1.09004        0.869\n",
      "            13          -1.08926        0.869\n",
      "            14          -1.08849        0.869\n",
      "            15          -1.08772        0.869\n",
      "            16          -1.08695        0.870\n",
      "            17          -1.08617        0.870\n",
      "            18          -1.08540        0.870\n",
      "            19          -1.08463        0.870\n",
      "            20          -1.08387        0.870\n",
      "            21          -1.08310        0.870\n"
     ]
    }
   ],
   "source": [
    "#Maxent train\n",
    "\n",
    "algorithm = nltk.classify.MaxentClassifier.ALGORITHMS[0]\n",
    "classifier = nltk.MaxentClassifier.train(train_set, algorithm, max_iter = 25)\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maxent testing on all banks and performance index calculation from classifier predictions\n",
    "\n",
    "values = {\n",
    "    \"compliment\": 1,\n",
    "    \"miscellaneous\": 0.5,\n",
    "    \"displeasure\": 0\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "direc = os.fsencode('./test')\n",
    "for File in os.listdir(direc):\n",
    "    filename = os.fsdecode(File)\n",
    "    try:\n",
    "        test = pd.read_csv(\"./test/\" + filename, header = None)\n",
    "        for i in range(len(test[0])):\n",
    "            test[0][i] = processTweet(test[0][i])\n",
    "            \n",
    "        x_test = clean_tweets(test)\n",
    "        y_test = test[1]\n",
    "\n",
    "        test_set = extract_feats(x_test, y_test)\n",
    "        y_true = y_test\n",
    "        y_pred = [classifier.classify(i[0]) for i in test_set]\n",
    "        \n",
    "        sum = 0\n",
    "        for i in y_pred:\n",
    "            sum += values[i]\n",
    "        result = {}\n",
    "        result['performance_index'] = sum * 100 / len(y_pred)\n",
    "        result['classification_report'] = classification_report(y_true, y_pred)\n",
    "        result['tweets'] = len(y_pred)\n",
    "        results[filename[:-4]] = result\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for i in sorted(results.items(), key = lambda x: ( -(getitem(x[1],'tweets')), -(getitem(x[1],'performance_index')) )):\n",
    "    print((i[0] + \": \").rjust(40) + (\"{:.2f}\".format(i[1]['performance_index'])).rjust(6) + \"% in \" + str(i[1]['tweets']).rjust(4) + \" tweets\")\n",
    "    print(i[1]['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i give up"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
