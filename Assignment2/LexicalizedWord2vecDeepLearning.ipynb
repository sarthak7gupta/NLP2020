{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import keras\n",
    "from keras import utils\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import multiprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "from operator import getitem\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'up', 'down', 'these', 'and', 'about', 'him', 'we', 'himself', 'to', 'if', 'where', 'only', \"didn't\", 'how', \"she's\", 'whom', 'an', 'ourselves', 'myself', 'but', 'your', \"should've\", 'what', 'few', 'do', 'have', 'each', 'themselves', 'or', 'm', \"you'd\", 'on', 'because', 'there', 'will', 'me', 'it', 'yourselves', 's', 'too', 'before', 'own', 'from', 'should', 'my', 'can', 'am', 'doing', 'is', 'further', 'until', 'which', 'into', 'again', 'such', 'ma', 'you', 'yours', 'he', 'does', 'very', 'just', 'out', 'a', 'she', 'his', 'who', 'while', 'are', 'over', 're', 'they', 'other', 'at', 'same', 'did', 'some', 'having', 'of', 'more', 'then', 'theirs', 'y', 'most', 'as', 'during', 'here', 'in', 've', 'herself', 'now', 'this', 'being', 'yourself', 'has', 'hers', 'for', 'll', 'any', 'i', 'itself', \"you'll\", 't', \"it's\", 'their', 'were', 'with', 'that', \"you're\", 'through', \"you've\", 'after', 'so', \"that'll\", 'o', 'why', 'than', 'off', 'ours', 'both', 'once', 'them', 'd', 'been', 'our', 'all', 'those', 'above', 'by', 'between', 'under', 'when', 'the', 'had', 'its', 'was', 'her', 'be', 'below'}\n"
     ]
    }
   ],
   "source": [
    "stopwords_set = set(stopwords.words('english'))\n",
    "required = {\"needn\", \"doesn\", \"won\", \"shan't\", \"don\", \"ain\", \"not\", \"didn\", \"hadn\", \"haven't\", \"couldn't\", \"wasn't\", \"aren't\", \"isn\", \"needn't\", \"aren\", \"wouldn\", \"shouldn\", \"hasn't\", \"shan\", \"no\", \"wasn\", \"nor\", \"hasn\", \"mightn\", \"doesn't\", \"against\", \"wouldn't\", \"couldn\", \"hadn't\", \"isn't\", \"mustn\", \"don't\", \"weren't\", \"haven\", \"mustn't\", \"shouldn't\", \"weren\", \"won't\", \"mightn't\"}\n",
    "stopwords_set -= required\n",
    "print(stopwords_set)\n",
    "dictionary = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"FinalTweetList_train.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncompliment = df[df[1]=='compliment']\\ndispleasure = df[df[1]=='displeasure']\\nmiscellaneous = df[df[1]=='miscellaneous']\\nsize = min(len(displeasure), len(miscellaneous))\\nprint(size)\\ncompliment_extra = pd.DataFrame()\\nwhile(len(compliment) + len(compliment_extra) < size):\\n    compliment_extra = compliment_extra.append(compliment.sample(min(size - (len(compliment) + len(compliment_extra)),len(compliment))))\\ncompliment = compliment_extra.append(compliment)\\n\\ndf = compliment.append([displeasure, miscellaneous])\\ndf = df.reset_index(drop = True)\\nprint(df)\\n\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#upsample compliments to equalise classes\n",
    "'''\n",
    "compliment = df[df[1]=='compliment']\n",
    "displeasure = df[df[1]=='displeasure']\n",
    "miscellaneous = df[df[1]=='miscellaneous']\n",
    "size = min(len(displeasure), len(miscellaneous))\n",
    "print(size)\n",
    "compliment_extra = pd.DataFrame()\n",
    "while(len(compliment) + len(compliment_extra) < size):\n",
    "    compliment_extra = compliment_extra.append(compliment.sample(min(size - (len(compliment) + len(compliment_extra)),len(compliment))))\n",
    "compliment = compliment_extra.append(compliment)\n",
    "\n",
    "df = compliment.append([displeasure, miscellaneous])\n",
    "df = df.reset_index(drop = True)\n",
    "print(df)\n",
    "'''\n",
    "# partially stratified upsample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       0              1\n",
      "0      at_user at_user at_user at_user at_user at_use...     compliment\n",
      "1      rt at_user at_user at_user at_user at_user at_...  miscellaneous\n",
      "2      first hand experience of the cashrush process ...     compliment\n",
      "3      at_user at_user hi, sorry for the trouble. we'...  miscellaneous\n",
      "4               at_user at_user the link is not opening.  miscellaneous\n",
      "...                                                  ...            ...\n",
      "11374  at_user in queue since 7 am at_user out of cur...  miscellaneous\n",
      "11375  at_user at_user at_user at_user at_user at_use...  miscellaneous\n",
      "11376  thank you bankers for your kind co operation. ...     compliment\n",
      "11377  hell lot of sms by at_user at_user but no atms...     compliment\n",
      "11378  at_user at_user at_user at_user atps not worki...    displeasure\n",
      "\n",
      "[11379 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def processTweet(tweet):\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    return tweet\n",
    "\n",
    "for i in range(len(df[0])):\n",
    "    df[0][i] = processTweet(df[0][i])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11379\n"
     ]
    }
   ],
   "source": [
    "#Stopword removal, non-alphabet tokens removed, optional non-english word removal\n",
    "\n",
    "#train, test = train_test_split(df, test_size = (500 if len(feats)>500 else 0.25), stratify = df[1], random_state = 21)\n",
    "#train, test = train_test_split(df, test_size = (500 if len(feats)>500 else 0.25), random_state = 21)\n",
    "#train = train.reset_index(drop = True)\n",
    "#test = test.reset_index(drop = True)\n",
    "train = df\n",
    "\n",
    "def word_clean(words):\n",
    "    #return ' '.join([word for word in words if (word.isalpha())])\n",
    "    #return ' '.join([word for word in words if (word.isalpha() and word in dictionary)])\n",
    "    #return ' '.join([word for word in words if (word.isalpha() and word not in stopwords_set)])\n",
    "    return ' '.join([word for word in words if (word.isalpha() and word not in stopwords_set and word in dictionary)])\n",
    "    \n",
    "def clean_tweets(dataframe):\n",
    "    return [word_clean(WordPunctTokenizer().tokenize(dataframe[0][i])) for i in range(len(dataframe[0]))]\n",
    "\n",
    "x_train = clean_tweets(train)\n",
    "y_train = train[1]\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3131\n",
      "Word2Vec trained\n",
      "LabelEncoder\n",
      "Fit Tokenizer\n",
      "Number of unique words: 3132\n",
      "Create Embedding matrix\n",
      "Embedding matrix: (3132, 300)\n",
      "Build Keras model\n",
      "x_train shape: (11379, 500)\n",
      "y_train shape: (11379, 3)\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 300)          939600    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 1,186,195\n",
      "Trainable params: 246,595\n",
      "Non-trainable params: 939,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fit Keras model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:998: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "11379/11379 [==============================] - 485s 43ms/step - loss: 0.8551 - accuracy: 0.5842\n",
      "Epoch 2/32\n",
      "11379/11379 [==============================] - 555s 49ms/step - loss: 0.6972 - accuracy: 0.6929\n",
      "Epoch 3/32\n",
      "11379/11379 [==============================] - 452s 40ms/step - loss: 0.6406 - accuracy: 0.7241\n",
      "Epoch 4/32\n",
      "11379/11379 [==============================] - 300s 26ms/step - loss: 0.5832 - accuracy: 0.7516\n",
      "Epoch 5/32\n",
      "11379/11379 [==============================] - 299s 26ms/step - loss: 0.5305 - accuracy: 0.7804\n",
      "Epoch 6/32\n",
      "11379/11379 [==============================] - 360s 32ms/step - loss: 0.4793 - accuracy: 0.8038\n",
      "Epoch 7/32\n",
      "11379/11379 [==============================] - 339s 30ms/step - loss: 0.4386 - accuracy: 0.8252\n",
      "Epoch 8/32\n",
      "11379/11379 [==============================] - 299s 26ms/step - loss: 0.3964 - accuracy: 0.8441\n",
      "Epoch 9/32\n",
      "11379/11379 [==============================] - 321s 28ms/step - loss: 0.3656 - accuracy: 0.8546\n",
      "Epoch 10/32\n",
      "11379/11379 [==============================] - 308s 27ms/step - loss: 0.3384 - accuracy: 0.8677\n",
      "Epoch 11/32\n",
      "11379/11379 [==============================] - 301s 26ms/step - loss: 0.3185 - accuracy: 0.8781\n",
      "Epoch 12/32\n",
      "11379/11379 [==============================] - 315s 28ms/step - loss: 0.3180 - accuracy: 0.8819\n",
      "Epoch 13/32\n",
      "11379/11379 [==============================] - 306s 27ms/step - loss: 0.2956 - accuracy: 0.8895\n",
      "Epoch 14/32\n",
      "11379/11379 [==============================] - 304s 27ms/step - loss: 0.2683 - accuracy: 0.8992\n",
      "Epoch 15/32\n",
      "11379/11379 [==============================] - 306s 27ms/step - loss: 0.2524 - accuracy: 0.9059\n",
      "Epoch 16/32\n",
      "11379/11379 [==============================] - 306s 27ms/step - loss: 0.2435 - accuracy: 0.9110\n",
      "Epoch 17/32\n",
      "11379/11379 [==============================] - 307s 27ms/step - loss: 0.2287 - accuracy: 0.9133\n",
      "Epoch 18/32\n",
      "11379/11379 [==============================] - 307s 27ms/step - loss: 0.2199 - accuracy: 0.9161\n",
      "Epoch 19/32\n",
      "11379/11379 [==============================] - 295s 26ms/step - loss: 0.2106 - accuracy: 0.9215\n",
      "Epoch 20/32\n",
      "11379/11379 [==============================] - 279s 24ms/step - loss: 0.1997 - accuracy: 0.9211\n",
      "Epoch 21/32\n",
      "11379/11379 [==============================] - 278s 24ms/step - loss: 0.1926 - accuracy: 0.9286\n",
      "Epoch 22/32\n",
      "11379/11379 [==============================] - 279s 24ms/step - loss: 0.1866 - accuracy: 0.9274\n",
      "Epoch 23/32\n",
      "11379/11379 [==============================] - 278s 24ms/step - loss: 0.1815 - accuracy: 0.9335\n",
      "Epoch 24/32\n",
      "11379/11379 [==============================] - 277s 24ms/step - loss: 0.1757 - accuracy: 0.9337\n",
      "Epoch 25/32\n",
      "11379/11379 [==============================] - 278s 24ms/step - loss: 0.1669 - accuracy: 0.9370\n",
      "Epoch 26/32\n",
      "11379/11379 [==============================] - 279s 25ms/step - loss: 0.1646 - accuracy: 0.9380\n",
      "Epoch 27/32\n",
      "11379/11379 [==============================] - 279s 24ms/step - loss: 0.1610 - accuracy: 0.9413\n",
      "Epoch 28/32\n",
      "11379/11379 [==============================] - 279s 25ms/step - loss: 0.1533 - accuracy: 0.9408\n",
      "Epoch 29/32\n",
      "11379/11379 [==============================] - 278s 24ms/step - loss: 0.1444 - accuracy: 0.9451\n",
      "Epoch 30/32\n",
      "11379/11379 [==============================] - 279s 25ms/step - loss: 0.1451 - accuracy: 0.9456\n",
      "Epoch 31/32\n",
      "11379/11379 [==============================] - 278s 24ms/step - loss: 0.1452 - accuracy: 0.9463\n",
      "Epoch 32/32\n",
      "11379/11379 [==============================] - 281s 25ms/step - loss: 0.1365 - accuracy: 0.9493\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "x_train = [gensim.utils.simple_preprocess(text) for text in x_train]\n",
    "#Continous bag of words used\n",
    "w2v_model = gensim.models.Word2Vec(min_count=1, window=5,\n",
    "                                        size=300,\n",
    "                                        workers=multiprocessing.cpu_count())\n",
    "w2v_model.build_vocab(x_train)\n",
    "w2v_model.train(x_train, total_examples=w2v_model.corpus_count, epochs=100)\n",
    "w2v_words = list(w2v_model.wv.vocab)\n",
    "print(\"Vocabulary size: %i\" % len(w2v_words))\n",
    "print(\"Word2Vec trained\")\n",
    "\n",
    "print(\"LabelEncoder\")\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "print(\"Fit Tokenizer\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(x_train),\n",
    "                                                     maxlen=500)\n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "print(\"Number of unique words: %i\" % num_words)\n",
    "\n",
    "print(\"Create Embedding matrix\")\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, idx in word_index.items():\n",
    "    if word in w2v_words:\n",
    "        embedding_vector = w2v_model.wv.get_vector(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = w2v_model.wv[word]\n",
    "print(\"Embedding matrix: %s\" % str(embedding_matrix.shape))\n",
    "\n",
    "print(\"Build Keras model\")\n",
    "print('x_train shape: %s' % str(x_train.shape))\n",
    "print('y_train shape: %s' % str(y_train.shape))\n",
    "\n",
    "k_model = Sequential()\n",
    "k_model.add(Embedding(vocab_size,\n",
    "                           300,\n",
    "                           weights=[embedding_matrix],\n",
    "                           input_length=500,\n",
    "                           trainable=False))\n",
    "k_model.add(LSTM(128, dropout=0.5, recurrent_dropout=0.2))\n",
    "for hidden_layer in (128, 64, 32):\n",
    "    k_model.add(Dense(hidden_layer, activation='relu'))\n",
    "    k_model.add(Dropout(0.2))\n",
    "if num_classes > 2:\n",
    "    k_model.add(Dense(num_classes, activation='softmax'))\n",
    "else:\n",
    "    k_model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "k_model.compile(loss='categorical_crossentropy' if num_classes > 2 else 'binary_crossentropy',\n",
    "                     optimizer='adam',\n",
    "                     metrics=['accuracy'])\n",
    "print(k_model.summary())\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='accuracy', patience=6, verbose=0, mode='max')\n",
    "rop = ReduceLROnPlateau(monitor='accuracy', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='max')\n",
    "callbacks = [early_stopping, rop]\n",
    "\n",
    "print(\"Fit Keras model\")\n",
    "k_model.fit(x_train, y_train,\n",
    "                 batch_size=128,\n",
    "                 epochs=32,\n",
    "                 callbacks=callbacks,\n",
    "                 verbose=1)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Kotak Bank:  38.80% in  500 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       0.89      0.89      0.89       113\n",
      "  displeasure       0.88      0.85      0.87       232\n",
      "miscellaneous       0.81      0.85      0.83       155\n",
      "\n",
      "     accuracy                           0.86       500\n",
      "    macro avg       0.86      0.87      0.86       500\n",
      " weighted avg       0.86      0.86      0.86       500\n",
      "\n",
      "                             Axis Bank:  34.20% in  500 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       0.90      0.93      0.92        41\n",
      "  displeasure       0.97      0.97      0.97       201\n",
      "miscellaneous       0.97      0.97      0.97       258\n",
      "\n",
      "     accuracy                           0.96       500\n",
      "    macro avg       0.95      0.95      0.95       500\n",
      " weighted avg       0.96      0.96      0.96       500\n",
      "\n",
      "                             HDFC Bank:  33.90% in  500 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       0.92      0.95      0.94        61\n",
      "  displeasure       0.99      0.94      0.97       234\n",
      "miscellaneous       0.93      0.97      0.95       205\n",
      "\n",
      "     accuracy                           0.95       500\n",
      "    macro avg       0.95      0.95      0.95       500\n",
      " weighted avg       0.96      0.95      0.95       500\n",
      "\n",
      "                   State Bank of India:  31.20% in  500 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       0.62      0.65      0.64        55\n",
      "  displeasure       0.74      0.76      0.75       238\n",
      "miscellaneous       0.71      0.67      0.69       207\n",
      "\n",
      "     accuracy                           0.71       500\n",
      "    macro avg       0.69      0.70      0.69       500\n",
      " weighted avg       0.71      0.71      0.71       500\n",
      "\n",
      "                            ICICI Bank:  29.30% in  500 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       0.97      0.98      0.98        65\n",
      "  displeasure       1.00      0.98      0.99       278\n",
      "miscellaneous       0.97      0.99      0.98       157\n",
      "\n",
      "     accuracy                           0.98       500\n",
      "    macro avg       0.98      0.99      0.98       500\n",
      " weighted avg       0.98      0.98      0.98       500\n",
      "\n",
      "                 Reserve Bank of India:  28.26% in  253 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       0.91      0.95      0.93        22\n",
      "  displeasure       0.96      0.94      0.95       136\n",
      "miscellaneous       0.92      0.94      0.93        95\n",
      "\n",
      "     accuracy                           0.94       253\n",
      "    macro avg       0.93      0.94      0.94       253\n",
      " weighted avg       0.94      0.94      0.94       253\n",
      "\n",
      "                        Bank of Baroda:  37.32% in  138 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       0.79      0.83      0.81        23\n",
      "  displeasure       0.76      0.80      0.78        56\n",
      "miscellaneous       0.82      0.76      0.79        59\n",
      "\n",
      "     accuracy                           0.79       138\n",
      "    macro avg       0.79      0.80      0.79       138\n",
      " weighted avg       0.79      0.79      0.79       138\n",
      "\n",
      "                              Yes Bank:  42.11% in   19 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       1.00      1.00      1.00         4\n",
      "  displeasure       1.00      1.00      1.00         7\n",
      "miscellaneous       1.00      1.00      1.00         8\n",
      "\n",
      "     accuracy                           1.00        19\n",
      "    macro avg       1.00      1.00      1.00        19\n",
      " weighted avg       1.00      1.00      1.00        19\n",
      "\n",
      "                  Punjab National Bank:  36.84% in   19 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       1.00      1.00      1.00         4\n",
      "  displeasure       0.78      1.00      0.88         7\n",
      "miscellaneous       1.00      0.75      0.86         8\n",
      "\n",
      "     accuracy                           0.89        19\n",
      "    macro avg       0.93      0.92      0.91        19\n",
      " weighted avg       0.92      0.89      0.89        19\n",
      "\n",
      "                         IndusInd Bank:  34.62% in   13 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       1.00      1.00      1.00         1\n",
      "  displeasure       1.00      1.00      1.00         5\n",
      "miscellaneous       1.00      1.00      1.00         7\n",
      "\n",
      "     accuracy                           1.00        13\n",
      "    macro avg       1.00      1.00      1.00        13\n",
      " weighted avg       1.00      1.00      1.00        13\n",
      "\n",
      "                             IDBI Bank:  40.91% in   11 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       1.00      1.00      1.00         3\n",
      "  displeasure       0.80      1.00      0.89         4\n",
      "miscellaneous       1.00      0.75      0.86         4\n",
      "\n",
      "     accuracy                           0.91        11\n",
      "    macro avg       0.93      0.92      0.92        11\n",
      " weighted avg       0.93      0.91      0.91        11\n",
      "\n",
      "                              Citibank:  18.18% in   11 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       1.00      1.00      1.00         1\n",
      "  displeasure       1.00      1.00      1.00         8\n",
      "miscellaneous       1.00      1.00      1.00         2\n",
      "\n",
      "     accuracy                           1.00        11\n",
      "    macro avg       1.00      1.00      1.00        11\n",
      " weighted avg       1.00      1.00      1.00        11\n",
      "\n",
      "                         Bank of India:  25.00% in    6 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  displeasure       1.00      1.00      1.00         3\n",
      "miscellaneous       1.00      1.00      1.00         3\n",
      "\n",
      "     accuracy                           1.00         6\n",
      "    macro avg       1.00      1.00      1.00         6\n",
      " weighted avg       1.00      1.00      1.00         6\n",
      "\n",
      "                                 PayTM:  40.00% in    5 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       1.00      1.00      1.00         1\n",
      "  displeasure       1.00      1.00      1.00         2\n",
      "miscellaneous       1.00      1.00      1.00         2\n",
      "\n",
      "     accuracy                           1.00         5\n",
      "    macro avg       1.00      1.00      1.00         5\n",
      " weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "                           Vijaya Bank:  50.00% in    4 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       1.00      1.00      1.00         1\n",
      "  displeasure       1.00      1.00      1.00         1\n",
      "miscellaneous       1.00      1.00      1.00         2\n",
      "\n",
      "     accuracy                           1.00         4\n",
      "    macro avg       1.00      1.00      1.00         4\n",
      " weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "                           Canara Bank:  50.00% in    3 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   compliment       0.00      0.00      0.00         1\n",
      "miscellaneous       0.67      1.00      0.80         2\n",
      "\n",
      "     accuracy                           0.67         3\n",
      "    macro avg       0.33      0.50      0.40         3\n",
      " weighted avg       0.44      0.67      0.53         3\n",
      "\n",
      "                      Corporation Bank:  33.33% in    3 tweets\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  compliment       1.00      1.00      1.00         1\n",
      " displeasure       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         3\n",
      "   macro avg       1.00      1.00      1.00         3\n",
      "weighted avg       1.00      1.00      1.00         3\n",
      "\n",
      "                           Post Office:  33.33% in    3 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  displeasure       1.00      1.00      1.00         1\n",
      "miscellaneous       1.00      1.00      1.00         2\n",
      "\n",
      "     accuracy                           1.00         3\n",
      "    macro avg       1.00      1.00      1.00         3\n",
      " weighted avg       1.00      1.00      1.00         3\n",
      "\n",
      "                   Union Bank of India:  16.67% in    3 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  displeasure       1.00      1.00      1.00         2\n",
      "miscellaneous       1.00      1.00      1.00         1\n",
      "\n",
      "     accuracy                           1.00         3\n",
      "    macro avg       1.00      1.00      1.00         3\n",
      " weighted avg       1.00      1.00      1.00         3\n",
      "\n",
      "                        Allahabad Bank:  50.00% in    2 tweets\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  compliment       1.00      1.00      1.00         1\n",
      " displeasure       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "                          Federal Bank:  50.00% in    2 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "miscellaneous       1.00      1.00      1.00         2\n",
      "\n",
      "     accuracy                           1.00         2\n",
      "    macro avg       1.00      1.00      1.00         2\n",
      " weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "                                  HSBC:  50.00% in    2 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "miscellaneous       1.00      1.00      1.00         2\n",
      "\n",
      "     accuracy                           1.00         2\n",
      "    macro avg       1.00      1.00      1.00         2\n",
      " weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "National Payments Corporation of India:  50.00% in    2 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "miscellaneous       1.00      1.00      1.00         2\n",
      "\n",
      "     accuracy                           1.00         2\n",
      "    macro avg       1.00      1.00      1.00         2\n",
      " weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "                         Oriental Bank:  50.00% in    2 tweets\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  compliment       1.00      1.00      1.00         1\n",
      " displeasure       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "                     Banking Ombudsman:  25.00% in    2 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  displeasure       1.00      1.00      1.00         1\n",
      "miscellaneous       1.00      1.00      1.00         1\n",
      "\n",
      "     accuracy                           1.00         2\n",
      "    macro avg       1.00      1.00      1.00         2\n",
      " weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "                              UCO Bank:  25.00% in    2 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  displeasure       1.00      1.00      1.00         1\n",
      "miscellaneous       1.00      1.00      1.00         1\n",
      "\n",
      "     accuracy                           1.00         2\n",
      "    macro avg       1.00      1.00      1.00         2\n",
      " weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "                          Central Bank:   0.00% in    2 tweets\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " displeasure       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "                       IDFC First Bank:  50.00% in    1 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "miscellaneous       1.00      1.00      1.00         1\n",
      "\n",
      "     accuracy                           1.00         1\n",
      "    macro avg       1.00      1.00      1.00         1\n",
      " weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "                  Indian Overseas Bank:  50.00% in    1 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "miscellaneous       1.00      1.00      1.00         1\n",
      "\n",
      "     accuracy                           1.00         1\n",
      "    macro avg       1.00      1.00      1.00         1\n",
      " weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "                              MobiKwik:  50.00% in    1 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "miscellaneous       1.00      1.00      1.00         1\n",
      "\n",
      "     accuracy                           1.00         1\n",
      "    macro avg       1.00      1.00      1.00         1\n",
      " weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "                            World Bank:  50.00% in    1 tweets\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "miscellaneous       1.00      1.00      1.00         1\n",
      "\n",
      "     accuracy                           1.00         1\n",
      "    macro avg       1.00      1.00      1.00         1\n",
      " weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "                   Bank of Maharashtra:   0.00% in    1 tweets\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " displeasure       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "                              DCB Bank:   0.00% in    1 tweets\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " displeasure       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare test\n",
    "\n",
    "values = {\n",
    "    \"compliment\": 1,\n",
    "    \"miscellaneous\": 0.5,\n",
    "    \"displeasure\": 0\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "direc = os.fsencode('./test')\n",
    "for File in os.listdir(direc):\n",
    "    filename = os.fsdecode(File)\n",
    "    try:\n",
    "        test = pd.read_csv(\"./test/\" + filename, header = None)\n",
    "        for i in range(len(test[0])):\n",
    "            test[0][i] = processTweet(test[0][i])\n",
    "        x_test = clean_tweets(test)\n",
    "        y_test = test[1]\n",
    "\n",
    "        x_test = [gensim.utils.simple_preprocess(text) for text in x_test]\n",
    "        x_test = keras.preprocessing.sequence.pad_sequences(\n",
    "            tokenizer.texts_to_sequences(x_test),\n",
    "            maxlen=500)\n",
    "\n",
    "        # Predict\n",
    "        confidences = k_model.predict(x_test, verbose=0)\n",
    "\n",
    "        y_pred_1d = []\n",
    "\n",
    "        for confidence in confidences:\n",
    "            idx = np.argmax(confidence)\n",
    "            y_pred_1d.append(label_encoder.classes_[idx])\n",
    "            \n",
    "        sum = 0\n",
    "        for i in y_pred_1d:\n",
    "            sum += values[i]\n",
    "        result = {}\n",
    "        result['performance_index'] = sum * 100 / len(y_pred_1d)\n",
    "        result['classification_report'] = classification_report(y_test, y_pred_1d)\n",
    "        result['tweets'] = len(y_pred_1d)\n",
    "        results[filename[:-4]] = result\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "for i in sorted(results.items(), key = lambda x: ( -(getitem(x[1],'tweets')), -(getitem(x[1],'performance_index')) )):\n",
    "    print((i[0] + \": \").rjust(40) + (\"{:.2f}\".format(i[1]['performance_index'])).rjust(6) + \"% in \" + str(i[1]['tweets']).rjust(4) + \" tweets\")\n",
    "    print(i[1]['classification_report'])\n",
    "    \n",
    "#The toppers are:\n",
    "    #1. Kotak Bank: 38.80%\n",
    "    #2. Axis Bank: 34.20%\n",
    "    #3. HDFC Bank: 33.90%\n",
    "    #4. State Bank of India: 31.20%\n",
    "    #5. ICICI Bank: 29.30%\n",
    "#Absolutely correct order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
